
# 算法调参

## 网格搜索sklearn.model_selection.GridSearchCV
### GridSearchCV参数
+ estimator：分类器，要传入除搜索参数外的其它参数（与scoring参数一起使用）
+ param_grid：需要优化的参数的取值，值为字典或列表
+ scoring：模型评价指标，默认为None，这时需要使用score函数；或者scoring=’roc_auc’，根据所选模型不同，评价准则不同。字符串（函数名），或是可调用对象，需要其函数签名，形如：score(estimator, X, y)；如果是None，则使用estimator的误差估计函数。
+ n_jobs：并行数。None表示为1，-1表示使用所有cpu核。
+ cv：交叉验证参数，默认为None，使用3折交叉验证。
+ verbose: 决定建模完成后对输出的打印方式。
	+ 0：不输出任何结果（默认）
	+ 1：打印特定区域的树的输出结果
	+ \>1：打印所有结果
+ pre_dispatch：指定总共分发的并行任务数。当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM。而设置pre_dispatch参数可以预先划分总共的job数量，使数据最多被复制pre_dispatch次。
+ refit：默认为True,程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与开发集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。
+ iid：默认为True，设为True时，各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。

### GridSearchCV常用属性
+ best\_score\_：最佳模型下的份数
+ best\_params：最佳模型参数
+ cv\_results\_：模型不同参数下交叉验证的结果（替换了20版本前的grid_scores_）
+ best\_estimator：最佳分类器

### GridSearchCV常用函数
+ score(x_test, y_test)：最佳模型在测试集下的分数

## GBDT调参
+ **Boosting框架的重要参数**
	+ n_estimators：弱学习器的最大迭代次数。太小，容易欠拟合；太大，容易过拟合。通常与learning_rate一起调参。
	+ learning_rate：每个弱学习器的权重衰减系数，称作步长。
	+ subsample：子采样，取值为(0,1]。这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本使用，相当于没有使用子采样；如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5,0.8]之间，默认是1.0，即不采用子采样。
	+ init：初始化的学习器。如果对数据有先验知识，则需要设置该值，否则可以不用管这个参数。
	+ loss：GBDT算法中的损失函数。对于分类模型，有对数似然损失函数“deviance”（默认值，对二元分离和多元分类较好）和指数损失函数“exponential”（Adaboost算法）。对于回归模型，有均方差“ls”，绝对损失“lad”，Huber损失“huber”和分位数损失“quantile”。默认是均方差“ls”；如果数据噪点很多，推荐用抗噪音的损失函数“huber”；如果需要对训练集进行分段进行预测的时候，则采用“quantile”。
	+ alpha：GradientBoostingRegressor函数有。当使用Huber损失“huber”和分位数损失“quantile”时，需要指定分位数的值。默认为0.9，如果噪音点较多，可适当降低这个分位数的值。

+ **CART回归树的重要参数**
	+ max_features：划分时考虑的最大特征数。默认为“None”，意味着划分时考虑所有的特征数；如果为“log2”，意味着最多考虑log2N个特征；如果是整数，表示考虑的特征绝对数；如果是浮点数，表示考虑特征百分比，即考虑（百分比×N）取整后的特征数。如果特征数很多，可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。
	+ max_depth：决策树最大深度。默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。数据少或特征少的时候可以不管这个值。如果样本量大，特征量大的情况下，推荐限制这个最大深度，具体取值取决于数据的分布。常用的取值为10—100之间。
	+ min_samples_split：内部节点再划分所需最小样本数。这个值限制了子树继续划分的条件，如果节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。默认为2，如果样本量不大，可以不用管，如果样本量数量级非常大，则推荐增大这个值。
	+ min_samples_leaf：叶子节点最少样本数。这个值限制了叶子结点最少的样本数，如果某叶子结点数目小于样本数，则会和兄弟节点一起被剪枝。默认是1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需管这个值。如果样本量数量级非常大，则推荐增大这个值。
	+ min_weight_fraction_leaf：叶子节点最小的样本权重。这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。默认是0，表示不考虑权重问题。一般来说，如果有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时应该设置此值。
	+ max_leaf_nodes：最大叶子节点数。通过限制最大叶子节点数，可以防止过拟合，默认是“None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征数不多，可以不考虑这个值，否则，需加以限制，具体的值可以通过交叉验证得到。
	+ min_impurity_split：节点划分最小不纯度。这个值限制了决策树的增长，如果某节点的不纯度（基尼系数，均方差）小于这个阈值，则该节点不再生成子节点，即为叶子节点，一般不推荐改动默认值1e-7。

+ **其它参数**
	+ verbose：决定建模完成后对输出的打印方式。
		+ 0：不输出任何结果（默认）
		+ 1：打印特定区域的树的输出结果
		+ \>1：打印所有结果
	+ warm_start：当设置为True时，重复使用之前的解法去拟合以及增加更多的学习器用于集成。否则的话，清除之前的解法。
	+ presort：决定是否对数据进行预排序，可以使得树分裂地更快。默认情况下是自动选择。

### GBDT调参过程

	1. 首先，用默认参数拟合数据，查看结果。
	2. 利用经验值初始化设置参数。设置一个较小的步长learning_rate来网格搜索最好的迭代次数n_estimators。
	3. 确定好较小的步长learning_rate和迭代次数n_estimators之后，网格搜索决策树的最大深度max_depth和内部节点再划分所需最小样本数min_samples_split。
	4. 确定好最大深度max_depth（在合理的范围之类），此时暂时不能确定min_samples_split，因为它和其它参数有关。网格搜索内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。
	5. 确定好内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf。网格搜索最大特征数max_features。
	6. 确定好最大特征数max_features。网格搜索子采样比subsample。
	7. 确定好子采样比subsample。将步长learning_rate减n倍，最大迭代次数n_estimators增加n倍来寻找能够增加模型泛化能力的最优解。


## XGBoost调参
> **三种参数类型**：
>	+ General Parameters：调控整个方程
>	+ Booster Parameters：调控每步树的相关变量
>	+ Learning Task Parameters：调控优化表现的变量

+ **通用参数**
	+ booster：迭代的模型，gbtree（基于树的模型，默认值）和gbliner（线性模型）
	+ silent：已被移除。使用verbosity参数代替。
	+ verbosity：打印输出信息
		+ 0：不打印任何信息
		+ 1：打印警告信息
		+ 2：打印输出信息
		+ 3：debug信息
	+ n_thread：并行任务数

+ **Booster 参数**

	+ tree booster的表现总是优于linear booster，下面讨论tree booster参数

		* eta：与GBM中和学习率的概率相似。通过减少每一步的权重来使得boosting过程更加鲁棒。范围为(0, 1]，默认值为0.3，通常最终的值的范围为[0.01, 0.2]
		* gamma：定义一个节点进行分裂的最小loss function。gamma值越大，模型越鲁棒。
		* max_depth：树的最大深度。树越深，模型越复杂且易overfit。一般范围[3, 10]
		* min_child_weight：定义观测叶子节点中所有样本权重之和的最小值。如果树分裂导致叶子节点的样本权重之和少于min_child_weight，则分裂停止。在线性回归任务中，这个仅仅对应于每个节点所需的最小样本数。min_child_weight值越大，模型越鲁棒。
		* max_delta_step：定义每棵树的权重变化的最大步长，该参数使得更新更加平缓。值为0表示没有约束；值大于0，权重的变化将更加保守。
		* subsample：和GBM中一样。控制生成每棵树的数据采样比例。值越大，模型易overfit。通常取值为[0.5, 1]。
		* colsample_bytree：列采样，和GBM中max_features类似。控制生成每棵树所需的特征数。通常取值为[0.5, 1]。
		* colsample_bylevel：控制节点分裂所需的特征数。效果和前两个参数共同作用一样。
		* lambda：权重的L2正则化。值越大，越不容易过拟合。
		* alpha：权重的L1正则化。值越大，越不容易过拟合。
		* scale_pos_weight：值大于0时，在类别样本不平衡时，有助于快速收敛。典型值设置：负样本数总和/正样本数总和。

+ **Learning Task 参数**
	+ objective：定义最小化损失函数类型。常用值有：
		+ binary：logistic 二分类的逻辑回归，返回预测的概率
		+ multi：softmax softmax多分类器，返回预测的概率，需要设置num_class
		+ multi：softprob 和softmax一样，但返回的是每个数据属于各个类别的概率
	+ eval_metric：用于衡量验证数据的评价标准。
	+ seed：随机数的种子，设置它可以复现随机数据的结果，也可用于调整参数。

<font color=red size=4>注意：有两种XGBoost：</font>

	1. xgboost：直接引用xgboost，会用到其中的“CV”函数。
	2. XGBClassifier：xgboost的sklearn包。允许像GBM一样使用GridSearch和并行处理。

```python
def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):

	'''
	XGBoost模型并进行交叉验证：modelfit函数
	
	参数说明：alg为定义的XGBClassifier分类器
		     dtrain为训练数据集
		     predictors为所有特征属性名称
		     target为类别名称
	'''

	if useTrainCV:
		# 获取XGBClassifier分类器中xgboost类型的所有参数
		xgb_param = alg.get_xgb_params()
		# 构建xgboost自定义的数据矩阵类DMatrix，能优化存储和运算速度
		xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)
		cvresult = xgb.cv(xgb_param, xgtrain,num_boost_round=alg.get_params()['n_estimators'],nfold=cv_folds,metrics='auc',early_stopping_rounds=early_stopping_rounds, show_progress=False)
		alg.set_params(n_estimators=cvresult.shape[0])  # 设置迭代器的个数

	#Fit the algorithm on the data
	alg.fit(dtrain[predictors], dtrain[target],eval_metric='auc')

	#Predict training set:
	dtrain_predictions = alg.predict(dtrain[predictors])
	dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]

	#Print model report:
	print "\nModel Report"
	print "Accuracy : %.4g" % metrics.accuracy_score(dtrain[target].values, dtrain_predictions)
	print "AUC Score (Train): %f" % metrics.roc_auc_score(dtrain[target], dtrain_predprob)

	feat_imp = pd.Series(alg.feature_importances_).sort_values(ascending=False)
	feat_imp.plot(kind='bar', title='Feature Importances')
	plt.ylabel('Feature Importance Score')
	plt.show()
```

### XGBoost调参过程
+ **参数调优的一般方法：**
	+ 选择较高的学习速率，一般位于[0.05, 0.3]之间，选择对应于此学习速率的理想决策树数量。
	+ 对于给定的学习速率和决策树数量，进行决策树特定参数调优（max_depth, min_child_weight, gamma, subsample, colsample_bytree）。
	+ xgboost的正则化参数调优（lambda, alpha）。
	+ 降低学习速率，确定理想参数。

+ **调参过程**
	
	>1、为了确定boosting参数，先设置初始值
	>>learning_rate=0.1<br>
	>>n_estimators=1000  # n_estimators的上限值<br>
	>>max_depth=5，起始值在4~6之间都是不错的选择<br>
	>>min_child_weight=1, 这里选了较小的值，因为是极不平衡分类问题，因此某些叶子节点下的值会比较小<br>
	>>gamma=0，也可以选[0.1,0.2]之间的值<br>
	>>subsample=0.8, colsample_bytree=0.8，典型值在0.5-0.9之间<br>
	>>scale_pos_weight=1，这个值是因为类别十分不平衡<br>
	>>seed=27<br>
	>>在modelfit函数，利用xgboost中的cv函数来确定最佳的决策树数量n_estimators。

	>2、类似于GBM调参，使用GridSearchCV来调整max_depth和min_child_weight参数。<br>
	>3、类似于GBM调参，使用GridSearchCV来调整gamma参数。<br>
	>4、类似于GBM调参，使用GridSearchCV来调整subsample和colsample_bytree参数。<br>
	>5、类似于GBM调参，使用GridSearchCV来调整reg_alpha和reg_lambda参数。<br>
	>6、降低learning_rate，继续调整。



## LightGBM调参
### 相比XGBoost，LightGBM有如下优点：

	1. 更快的训练速度和更高的效率：LightGBM使用基于直方图的算法；LightGBM对增益最大的节点进行深入分解，避免了XGBoost对整层节点分裂法。
	2. 更低的内存占用：将连续的特征值分桶(buckets)装进离散的箱子(bins)
	3. 更高的准确率（与提升算法相比）：通过leaf-wise分裂方法产生比level-wise分裂方法更复杂的树，这就是实现更高准确率的主要因素。有时候会overfitting，但是可以通过设置max-depth参数来防止overfitting。
	4. 大数据处理能力：相比于XGBoost，由于在训练时间上的缩减，它具有处理大数据的能力。
	5. 支持并行学习

### 核心参数介绍
>对于LightGBM：
>>+	 核心参数
>>+	学习控制参数
>>+	IO参数
>>+	目标参数
>>+	度量参数
>>+	网络参数
>>+	GPU参数
>>+	模型参数。

<font color=red >常修改的参数为核心参数、学习控制参数、度量参数等。</font>

+ **核心参数：**
	+ boosting_type：默认是gbdt。LGB里的boosting参数要比xgb多，有rf、dart、doss。
	+ num_thread：指定线程的个数，最好设置成实际CPU内核数。
	+ objective：也称application，指的是任务目标，默认为regression。
	+ valid：验证集选用。支持多个验证集，用逗号分割。
	+ learning_rate：梯度下降的步长，默认为0.1，一般设置成0.05~0.2之间
	+ num_leaves：一棵树上的叶子数，默认值为31。
	+ device_type：默认值为CPU。为树学习选择设备，建议较小的max_bin(如63)来获得更快的速度；

+ **学习控制参数：**
	+ feature_fraction：训练每棵树的特征比例，默认值为1.0，取值范围为(0.0, 1.0]。
	+ bagging_fraction：类似于feature_fraction，在不进行重采样的情况下随机选择部分数据，默认值为1.0，取值范围为(0.0, 1.0]。
	+ bagging_freq：bagging的频率，0表示禁用bagging，k表示每k次迭代执行bagging。
	+ lambda_l1：表示L1正则化，默认为0。
	+ lambda_l2：表示L2正则化，默认为0。
	+ cat_smooth：默认值为0，用于分类特征；可以降低噪声在分类特征中的影响，尤其对数据很少的类别。

+ **度量参数：**
	+ metric：用于验证集上的评价指标。支持多指标，使用逗号分隔，如[‘auc’,’map’]

### 调参类似于XGBoost。

### LightGBM官方推荐
>	For better accuracy

+ 小学习率，大迭代次数
+ 大num_leaves（也许会overfitting）
+ Cross Validation
+ 大数据集
+ Try DART——训练过程使用dropout。

>处理overfitting的方法：
>红色的为推荐的

+ small max_bin – feature – 分桶略微粗一些
+ small num_leaves – 不要在单棵树上分的太细
+ <font color=red >控制 min_data_in_leaf和min_sum_hessian_in_leaf – 确保叶子节点还有足够的数据</font>
+ Sub_sample – 在构建每棵树的时候，在data上做一些sample
+ <font color=red >Sub feature – 在构建每棵树的时候，在feature上做一些sample</font>
+ Bigger training data – 更多的训练数据
+ Lambda_l1、lambda_l2和min_gain_to_split – 使用正则化
+ Max_depth – 控制树的深度








### XGBoost vs LightGBM
|         | XGBoost   |  LightGBM  |
| :--------   | :-----  | :----  |
| 树生长算法      | <font color='red'>Level-wise</font><br>good for engineering<br>optimization<br>but not efficient to learn model    |   <font color='red'>Leaf-wise</font> with max depth limitation<br><font color='red'>get better trees with smaller computation cost, also can avoid overfitting</font>     |
|   Split search algorithm      |   预排序算法(Pre-sorted algorithm)   |   直方图算法(Histogram algorithm, 能降低存储代价和计算代价)   |
| memory cost        |    2\*feature\* data*4 Bytes    |  #feature\*#data\*1 Bytes (<font color='red'>8x smaller)  |
| Calculation of split gain        |   O(#data* #features)   |   O(#bin* #features)   |
| Cache-line aware optimization        |    n/a    |  <font color='red'>40%</font> speed-up on Higgs data  |
| Categorical feature support        |   n/a   |   <font color='red'>8x</font> speed-up on Expo data  |

##### XGBoost需要将categorical特征转换为one-hot形式，而LightGBM能够直接使用categorical特征作为输入。

##### LigthGBM 并行算法支持
* 特征并行化：适用于小数据和特征较多场景；
* 数据并行化：适用于大数据和特征较少场景；
* voting并行化：适用于大数据和特征较多场景；

##### 直方图算法
* Compression of feature
* 分桶的思想，Map continues values to discrete values(called "bin")：eg，[0,0.1) -> 0，[0.1,0.3) -> 1，...

##### leaf-wise 和 level-wise 
* leaf-wise：只对具有最大增益的结点进行分裂，容易overfitting，所以通过控制max-depth来避免；
* level-leaf：对所有同一层结点进行分裂，容易带来不必要的计算开销，比如对不需要进行分裂的结点进行分裂。







## CatBoost调参
>CatBoost源于Category和Boosting，该库可以很好地处理各种类别型数据，是一种能够很好的处理类别型特征的梯度提升算法库。

### 优点：
	1. 性能卓越：在性能方面可以匹敌任何先进的机器学习算法；
	2. 鲁棒性/强健性：减少对很多超参数调优的需求，并降低overfitting的机会，使得模型变得更加通用性；
	3. 易于使用：提供与scikit集成的python接口，以及R和命令行界面；
	4. 实用：可以处理类别型、数值型特征；
	5. 可扩展：支持自定义损失函数；

### 改进：
	1. 类别型特征：对Greedy Target-based Statistics方法进行改进，在它的基础上添加了先验分布项，减少噪声和低频率数据对于数据分布的影响，即解决训练数据集合测试数据集数据结构和分布不一样的问题。对于回归问题，先验项可取数据集的均值；对于二分类，先验项是正例的先验概率；
	2. 特征组合：将几个类别型特征的任意组合视为新特征；
	3. 重要的实现细节：用数字代替类别值的另一种方法是计算该类别值在数据集特征中出现的次数；
	4. 克服梯度偏差：和所有标准梯度提升算法一样，都是通过构建新树来拟合当前模型的梯度。然而，所有经典的提升算法都存在由有偏的点态梯度估计引起的过拟合问题。在每个步骤中使用的梯度都使用当前模型中的相同的数据点来估计，这导致估计梯度在特征空间的任何域中的分布与该域中梯度的真实分布相比发生了偏移，从而导致过拟合。许多利用GBDT技术的算法（例如，XGBoost、LightGBM），构建一棵树分为两个阶段：选择树结构和在树结构固定后计算叶子节点的值。为了选择最佳的树结构，算法通过枚举不同的分割，用这些分割构建树，对得到的叶子节点中计算值，然后对得到的树计算评分，最后选择最佳的分割。两个阶段叶子节点的值都是被当做梯度或牛顿步长的近似值来计算。CatBoost第一阶段采用梯度步长的无偏估计，第二阶段使用传统的GBDT方案执行。
	5. 快速评分：CatBoost使用oblivious树作为基本预测器。在这类树中，相同的分割准则在树的整个级别上使用。这种树是平衡的，不太容易过拟合。梯度提升oblivious树被成功地用于各种学习任务。在oblivious树中，每个叶子节点的索引可以被编码为长度等于树深度的二进制向量。这在CatBoost模型评估器中得到了广泛的应用：我们首先将所有浮点特征、统计信息和独热编码特征进行二值化，然后使用二进制特征来计算模型预测值。
	6. 基于GPU实现快速学习：1、密集的数值特征：对于任何GBDT算法而言，最大的难点之一就是搜索最佳分割。尤其是对于密集的数值特征数据集来说，该步骤是建立决策树时的主要计算负担。CatBoost使oblivious 决策树作为基础模型，并将特征离散化到固定数量的箱子中以减少内存使用。箱子的数量是算法的参数。因此，可以使用基于直方图的方法来搜索最佳分割。我们利用一个32位整数将多个数值型特征打包，规则：（1）存储二进制特征用1位，每个整数包括32个特征；（2）存储不超过15个值的特征用4位，每个整数包括8个特征；（3）存储其他特征用8位（不同值的个数最大是255），每个整数包括4个特征。就GPU内存使用而言，CatBoost至少与LightGBM一样有效。主要不同是利用一种不同的直方图计算方法。LightGBM和XGBoost的算法有一个主要缺点：它们依赖于原子操作。这种技术在内存上很容易处理，但是在性能好的GPU上，它会比较慢。事实上直方图可以在不涉及原子操作的情况下更有效地计算。2、类别型特征：CatBoost实现了多种处理类别型特征的方法。对于独热编码特征，我们不需要任何特殊处理，基于直方图的分割搜索方法可以很容易地处理这种情况。在数据预处理阶段，就可以对单个类别型特征进行统计计算。CatBoost还对特征组合使用统计信息。处理它们是算法中速度最慢、消耗内存最多的部分。CatBoost使用完美哈希来存储类别特征的值，以减少内存使用。由于GPU内存的限制，我们在CPU RAM中存储按位压缩的完美哈希，以及要求的数据流、重叠计算和内存等操作。动态地构造特征组合要求我们为这个新特征动态地构建（完美）哈希函数，并为哈希的每个唯一值计算关于某些排列的统计数据。CatBoost使用基数排序来构建完美的哈希，并通过哈希来分组观察。在每个组中，需要计算一些统计量的前缀和。该统计量的计算使用分段扫描GPU图元进行（CatBoost分段扫描实现通过算子变换完成，并且基于CUB中扫描图元的高效实现）。3、多GPU支持：CatBoost中的GPU实现可支持多个GPU。分布式树学习可以通过数据或特征进行并行化。CatBoost采用多个学习数据集排列的计算方案，在训练期间计算分类特征的统计数据。


### CatBoost参数详解
+ **通用参数：**
	+ loss_function：损失函数，支持的有：
		+ RMSE：默认值
		+ Logloss
		+ MAE
		+ CrossEntropy
		+ Quantile
		+ LogLinQuantile
		+ Multiclass
		+ MultiClassOneVsAll
		+ MAPE
		+ Poisson
	+ custom_metric：训练过程中输出的度量值。这些功能未经优化，仅出于信息目的显示。默认None。
	+ eval_metric：用于过拟合检验（设置True）和最佳模型选择（设置True）的loss function，用于优化。
	+ iterations：最大树数。默认1000。
	+ learning_rate：学习率。默认03。
	+ random_seed：训练时候的随机种子
	+ l2_leaf_reg：L2正则参数。默认3
	+ bootstrap_type：定义权重计算逻辑，可选参数：Poisson (supported for GPU only)/Bayesian/Bernoulli/No，默认为Bayesian
	+ bagging_temperature：贝叶斯套袋控制强度，区间[0, 1]。默认1。
	+ subsample：设置样本率，当bootstrap_type为Poisson或Bernoulli时使用，默认66
	+ sampling_frequency：设置创建树时的采样频率，可选值PerTree/PerTreeLevel，默认为PerTreeLevel
	+ random_strength：分数标准差乘数。默认1。
	+ use_best_model：设置此参数时，需要提供测试数据，树的个数通过训练参数和优化loss function获得。默认False。
	+ best_model_min_trees：最佳模型应该具有的树的最小数目。
	+ depth：树深，最大16，建议在1到10之间。默认6。
	+ ignored_features：忽略数据集中的某些特征。默认None。
	+ one_hot_max_size：如果feature包含的不同值的数目超过了指定值，将feature转化为float。默认False
	+ has_time：在将categorical features转化为numerical features和选择树结构时，顺序选择输入数据。默认False（随机）
	+ rsm：随机子空间（Random subspace method）。默认1。
	+ nan_mode：处理输入数据中缺失值的方法，包括Forbidden(禁止存在缺失)，Min(用最小值补)，Max(用最大值补)。默认Min。
	+ fold_permutation_block_size：数据集中的对象在随机排列之前按块分组。此参数定义块的大小。值越小，训练越慢。较大的值可能导致质量下降。
	+ leaf_estimation_method：计算叶子值的方法，Newton/ Gradient。默认Gradient。
	+ leaf_estimation_iterations：计算叶子值时梯度步数。
	+ leaf_estimation_backtracking：在梯度下降期间要使用的回溯类型。
	+ fold_len_multiplier folds：长度系数。设置大于1的参数，在参数较小时获得最佳结果。默认2。
	+ approx_on_full_history：计算近似值，
		+ False：使用1／fold_len_multiplier计算；
		+ True：使用fold中前面所有行计算。默认False。
	+ class_weights：类别的权重。默认None。
	+ scale_pos_weight：二进制分类中class 1的权重。该值用作class 1中对象权重的乘数。
	+ boosting_type：增压方案
	+ allow_const_label：使用它为所有对象训练具有相同标签值的数据集的模型。默认为False
+ **CatBoost默认参数：**
	+ iterations：1000,
	+ learning_rate：0.03,
	+ l2_leaf_reg：3,
	+ bagging_temperature：1,
	+ subsample：0.66,
	+ random_strength：1,
	+ depth：6,
	+ rsm：1,
	+ one_hot_max_size：2
	+ leaf_estimation_method：’Gradient’,
	+ fold_len_multiplier：2,
	+ border_count：128,
+ **CatBoost参数取值范围：**
	+ learning_rate：Log-uniform distribution [e^{-7}, 1]
	+ random_strength：Discrete uniform distribution over a set {1, 20}
	+ one_hot_max_size：Discrete uniform distribution over a set {0, 25}
	+ l2_leaf_reg：Log-uniform distribution [1, 10]
	+ bagging_temperature：Uniform [0, 1]
	+ gradient_iterations：Discrete uniform distribution over a set {1, 10}


